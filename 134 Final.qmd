---
title: "Final Project 134"
author: "Fion, Jessica, Dulce, Yikai"
date: "2025-02-27"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
editor_options: 
  markdown: 
    wrap: sentence
---

# **Introduction to the Recommendation System Project**

## Project Overview

In this project, our group aims to develop a recommendation system using an Amazon dataset to enhance user experience by providing personalized product recommendations.
With the vast amount of data available in e-commerce, recommendation systems play a crucial role in helping users discover relevant products efficiently.
By leveraging machine learning techniques, we aim to build a model that predicts user preferences based on historical interactions and product attributes.

## Data Description

We obtain our data set on Kalggles.
The data set contains **1,465** entries with **16 columns**, providing information about Amazon products, user reviews, and ratings.
We are focusing on the variables required for content filtering, which are **review_id**, **user_id**, **product_id**, and **rating**.
These variables will help us analyze user preferences and product recommendations effectively.
This data set provides valuable insights into consumer behavior, product popularity, and user preferences, making it an ideal choice for developing a recommendation system.

## Methodology

**Methodology Summary for Recommendation Systems**

There are two primary types of recommendation systems:

1\.
**Content-Based Filtering (Chosen for Our Project)**

-   Content-based recommendation systems use product attributes to suggest similar items to users.

-   Each product's features (e.g., category, price, description, and ratings) are transformed into numerical vectors.

-   The similarity between items is computed using **cosine similarity**, which measures how closely related two products are based on their feature vectors.

-   The system recommends items that are most similar to those the user has previously interacted with

2\.
**Collaborative Filtering**

-   This method relies on user interactions (ratings, reviews) to find patterns among users or items.

-   It can be **user-based**, where users with similar preferences are grouped together, or **item-based**, where products with similar rating patterns are recommended.

-   Cosine similarity is also used in this method to determine relationships between users or items.

# Exploring Our Data

## Loading Packages

```{r}
library(tidyverse)
library(dplyr)
<<<<<<< Updated upstream
library(tidytext)
library(tm)
library(wordcloud)
library(ggplot2)
library(igraph)
library(ggraph)
library(textdata)
library(reshape2)
library(word2vec)
library(umap)
library(plotly)
library(kableExtra)
=======
library(ggplot2)

>>>>>>> Stashed changes
```

## Loading in the Dataset

```{r}
# load the data set
amazon <- read.csv("data/amazon.csv")
```

## Exploring Raw Data

```{r}
summary(amazon)
colSums(is.na(amazon))
```

## Tidying the Data

```{r}
# Mutate character variables into numeric
amazon$discounted_price <- as.numeric(gsub("[^0-9.]", "", amazon$discounted_price))
amazon$actual_price <- as.numeric(gsub("[^0-9.]", "", amazon$actual_price))
amazon$discount_percentage <- as.numeric(gsub("[^0-9]", "", amazon$discount_percentage))
amazon$rating_count <- as.numeric(gsub("[^0-9.]", "", amazon$rating_count))

# Find non-numeric rating values
print(amazon$rating[grepl("[^0-9.]", amazon$rating)])

amazon <- amazon %>% 
  filter(grepl("^[0-9.]", rating)) %>% 
  filter(!is.na(rating_count)) %>% 
  mutate(rating = as.numeric(rating))

# Confirm no missing
colSums(is.na(amazon))
```

posssible pca, nlp usage

```{r}
summary(amazon)
```

## Visual EDA

### NLP graphs:
```{r}
# Select relevant columns and clean it
# there is no missing value
amazon <- amazon %>%
  select(review_title) %>%
  mutate(review_title = tolower(review_title) %>% str_replace_all('[[:punct:]]', ''))

# Tokenization
amazon_tokens <- amazon %>%
  unnest_tokens(word, review_title)


# Remove stop words
data("stop_words")

amazon_clean <- amazon_tokens %>%
  anti_join(stop_words, by = "word")

# Plot the most Common Words
top_words <- amazon_clean %>%
  count(word, sort = TRUE) %>%
  head(20)

ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(title = "Top 20 Most Common Words in Review Titles", x = "Words", y = "Frequency") +
  theme_minimal()

# Clean
amazon_clean %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2")))

# Sentiment Analysis
amazon_sentiment <- amazon_clean %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE)

amazon_sentiment %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"), max.words = 100)

# Most common Bigrams
amazon_bigrams <- amazon %>%
  unnest_tokens(bigram, review_title, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>%
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word)

amazon_bigrams %>%
  count(bigram, sort = TRUE) %>%
  head(20) %>%
  ggplot(aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "coral") +
  coord_flip() +
  labs(title = "Top 20 Most Common Bigrams", x = "Bigrams", y = "Count")

```